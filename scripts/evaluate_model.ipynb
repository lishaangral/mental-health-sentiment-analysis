{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f4f7863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick Accuracy on 200 samples: 91.5%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a few test samples\n",
    "df = pd.read_csv(\"../data/mental_health_dataset.csv\").sample(n=200, random_state=42)  # using 10â€“20 rows for quick check\n",
    "\n",
    "# Load model from Hugging Face\n",
    "model_name = \"lishaangral/roberta-mental-health\"\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"lishaangral/roberta-mental-health\",\n",
    "    tokenizer=\"lishaangral/roberta-mental-health\",\n",
    "    framework=\"pt\"  # Force PyTorch backend\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "preds = classifier(\n",
    "    list(df[\"statement\"]),\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "pred_labels = [p[\"label\"] for p in preds]\n",
    "\n",
    "# Evaluate\n",
    "true_labels = list(df[\"status\"])\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Quick Accuracy on {len(df)} samples: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46738ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Anxiety      0.917     1.000     0.957        11\n",
      "             Bipolar      1.000     0.833     0.909         6\n",
      "          Depression      0.897     0.897     0.897        68\n",
      "              Normal      1.000     0.985     0.992        66\n",
      "Personality disorder      1.000     1.000     1.000         4\n",
      "              Stress      0.727     0.889     0.800         9\n",
      "            Suicidal      0.829     0.806     0.817        36\n",
      "\n",
      "            accuracy                          0.915       200\n",
      "           macro avg      0.910     0.916     0.910       200\n",
      "        weighted avg      0.917     0.915     0.915       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate full classification report\n",
    "report = classification_report(true_labels, pred_labels, digits=3)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a19da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load GoEmotions test data\n",
    "test = load_dataset(\"go_emotions\", split=\"test\")\n",
    "\n",
    "# Load multi-label classifier pipeline\n",
    "model_name2 = \"lishaangral/roberta-mental-health-v2\"\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model_name2,\n",
    "    tokenizer=model_name2,\n",
    "    framework=\"pt\",\n",
    "    return_all_scores=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    top_k=None  # return scores for all labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3d36013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (micro): 0.0921\n",
      "Precision (micro): 0.1103\n",
      "Recall (micro): 0.0791\n",
      "Per-label Accuracy: 0.9359\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "sampled_test = test.shuffle(seed=42).select(range(1000))\n",
    "pred_outputs = classifier(list(sampled_test[\"text\"]))\n",
    "\n",
    "# Convert predictions to binary using threshold\n",
    "threshold = 0.5\n",
    "pred_labels_bin = []\n",
    "for scores in pred_outputs:\n",
    "    labels = [1 if s[\"score\"] > threshold else 0 for s in scores]\n",
    "    pred_labels_bin.append(labels)\n",
    "\n",
    "# Convert true labels to binary format (GoEmotions uses list of label indices)\n",
    "num_labels = len(pred_outputs[0])  # should be 28\n",
    "true_labels_bin = []\n",
    "for example_labels in sampled_test[\"labels\"]:\n",
    "    label_vec = [0] * num_labels\n",
    "    for idx in example_labels:\n",
    "        label_vec[idx] = 1\n",
    "    true_labels_bin.append(label_vec)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_true = np.array(true_labels_bin)\n",
    "y_pred = np.array(pred_labels_bin)\n",
    "\n",
    "# Evaluation\n",
    "f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "precision = precision_score(y_true, y_pred, average=\"micro\")\n",
    "recall = recall_score(y_true, y_pred, average=\"micro\")\n",
    "accuracy_per_label = (y_true == y_pred).mean()\n",
    "\n",
    "print(f\"F1 Score (micro): {f1:.4f}\")\n",
    "print(f\"Precision (micro): {precision:.4f}\")\n",
    "print(f\"Recall (micro): {recall:.4f}\")\n",
    "print(f\"Per-label Accuracy: {accuracy_per_label:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86399954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Micro-Averaged Scores ===\n",
      "F1 (micro):     0.5972\n",
      "Precision:      0.7152\n",
      "Recall:         0.5126\n",
      "\n",
      "=== Per-Label Metrics ===\n",
      "Label  0:  Acc: 0.9430 | F1: 0.7077 | Prec: 0.6571 | Recall: 0.7667\n",
      "Label  1:  Acc: 0.9800 | F1: 0.7959 | Prec: 0.7959 | Recall: 0.7959\n",
      "Label  2:  Acc: 0.9730 | F1: 0.4255 | Prec: 0.4762 | Recall: 0.3846\n",
      "Label  3:  Acc: 0.9570 | F1: 0.2712 | Prec: 0.8000 | Recall: 0.1633\n",
      "Label  4:  Acc: 0.9400 | F1: 0.4000 | Prec: 0.6250 | Recall: 0.2941\n",
      "Label  5:  Acc: 0.9840 | F1: 0.5000 | Prec: 0.7273 | Recall: 0.3810\n",
      "Label  6:  Acc: 0.9820 | F1: 0.5714 | Prec: 0.7059 | Recall: 0.4800\n",
      "Label  7:  Acc: 0.9590 | F1: 0.5393 | Prec: 0.7059 | Recall: 0.4364\n",
      "Label  8:  Acc: 0.9830 | F1: 0.2609 | Prec: 0.3750 | Recall: 0.2000\n",
      "Label  9:  Acc: 0.9710 | F1: 0.1212 | Prec: 0.5000 | Recall: 0.0690\n",
      "Label 10:  Acc: 0.9450 | F1: 0.2667 | Prec: 0.4167 | Recall: 0.1961\n",
      "Label 11:  Acc: 0.9820 | F1: 0.4706 | Prec: 0.7273 | Recall: 0.3478\n",
      "Label 12:  Acc: 0.9920 | F1: 0.5556 | Prec: 1.0000 | Recall: 0.3846\n",
      "Label 13:  Acc: 0.9810 | F1: 0.3871 | Prec: 0.6000 | Recall: 0.2857\n",
      "Label 14:  Acc: 0.9880 | F1: 0.6667 | Prec: 0.7500 | Recall: 0.6000\n",
      "Label 15:  Acc: 0.9890 | F1: 0.9134 | Prec: 0.9206 | Recall: 0.9062\n",
      "Label 16:  Acc: 0.9990 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
      "Label 17:  Acc: 0.9750 | F1: 0.6269 | Prec: 0.7500 | Recall: 0.5385\n",
      "Label 18:  Acc: 0.9830 | F1: 0.8283 | Prec: 0.8039 | Recall: 0.8542\n",
      "Label 19:  Acc: 0.9950 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
      "Label 20:  Acc: 0.9790 | F1: 0.6667 | Prec: 0.7778 | Recall: 0.5833\n",
      "Label 21:  Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
      "Label 22:  Acc: 0.9800 | F1: 0.1667 | Prec: 0.5000 | Recall: 0.1000\n",
      "Label 23:  Acc: 0.9990 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
      "Label 24:  Acc: 0.9940 | F1: 0.6250 | Prec: 0.7143 | Recall: 0.5556\n",
      "Label 25:  Acc: 0.9790 | F1: 0.5714 | Prec: 0.8235 | Recall: 0.4375\n",
      "Label 26:  Acc: 0.9810 | F1: 0.4865 | Prec: 0.6923 | Recall: 0.3750\n",
      "Label 27:  Acc: 0.7930 | F1: 0.6387 | Prec: 0.7093 | Recall: 0.5810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lisha Angral\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"lishaangral/roberta-mental-health-v2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Tokenize test texts\n",
    "inputs = tokenizer(\n",
    "    list(sampled_test[\"text\"]),\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Predict logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > 0.5).int().cpu().numpy()\n",
    "\n",
    "# Prepare true labels\n",
    "# Number of labels\n",
    "num_labels = model.config.num_labels  # Should be 28 for GoEmotions\n",
    "\n",
    "# Convert list of label indices to binary multi-hot encoded matrix\n",
    "true_labels = np.zeros((len(sampled_test), num_labels), dtype=int)\n",
    "for i, label_list in enumerate(sampled_test[\"labels\"]):\n",
    "    for label in label_list:\n",
    "        true_labels[i][label] = 1\n",
    "\n",
    "# Micro-averaged scores\n",
    "f1_micro = f1_score(true_labels, preds, average=\"micro\")\n",
    "precision_micro = precision_score(true_labels, preds, average=\"micro\")\n",
    "recall_micro = recall_score(true_labels, preds, average=\"micro\")\n",
    "\n",
    "print(f\"\\n=== Micro-Averaged Scores ===\")\n",
    "print(f\"F1 (micro):     {f1_micro:.4f}\")\n",
    "print(f\"Precision:      {precision_micro:.4f}\")\n",
    "print(f\"Recall:         {recall_micro:.4f}\")\n",
    "\n",
    "# Per-label metrics\n",
    "f1_per_label = f1_score(true_labels, preds, average=None)\n",
    "precision_per_label = precision_score(true_labels, preds, average=None)\n",
    "recall_per_label = recall_score(true_labels, preds, average=None)\n",
    "accuracy_per_label = (preds == true_labels).mean(axis=0)\n",
    "\n",
    "print(f\"\\n=== Per-Label Metrics ===\")\n",
    "for i in range(len(f1_per_label)):\n",
    "    print(f\"Label {i:2d}:  Acc: {accuracy_per_label[i]:.4f} | F1: {f1_per_label[i]:.4f} | Prec: {precision_per_label[i]:.4f} | Recall: {recall_per_label[i]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
